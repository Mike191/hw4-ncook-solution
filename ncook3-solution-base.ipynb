{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs7641assn4 as a4\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Board--\n",
      "\n",
      "\u001b[41mS\u001b[0m  F  F  F\n",
      "F  H  F  H\n",
      "F  F  F  H\n",
      "H  F  F  G\n",
      "\n",
      "--Actions for Position to the Left of the Goal--\n",
      "{0: [(1.0, 13, -0.2, False)],\n",
      " 1: [(1.0, 14, -0.2, False)],\n",
      " 2: [(1.0, 15, 1, True)],\n",
      " 3: [(1.0, 10, -0.2, False)]}\n",
      "\n",
      "--Reward Values at Each State--\n",
      "-0.2  -0.2  -0.2  -0.2  \n",
      "-0.2    -1  -0.2    -1  \n",
      "-0.2  -0.2  -0.2    -1  \n",
      "  -1  -0.2  -0.2     1  \n"
     ]
    }
   ],
   "source": [
    "env_id = 'Deterministic-4x4-FrozenLake-v0' # string identifier for environment, arbitrary label\n",
    "rH = -1 #-5 # reward for H(ole)\n",
    "rG = 1 # 10 # reward for G(oal)\n",
    "rF = -0.2# reward includes S(tart) and F(rozen)\n",
    "size = 4 # height and width of square gridworld\n",
    "p = 0.8 # if generating a random map probability that a grid will be F(rozen)\n",
    "desc = None # frozen_lake.generate_random_map(size=size, p=p)\n",
    "map_name = 'x'.join([str(size)]*2) # None\n",
    "is_slippery = False\n",
    "\n",
    "\n",
    "epsilon = 1e-8 # convergence threshold for policy/value iteration\n",
    "gamma = 0.8 # discount parameter for past policy/value iterations\n",
    "max_iter = 10000 # maximum iterations for slowly converging policy/value iteration \n",
    "\n",
    "# Qlearning(env, rH=0, rG=1, rF=0, qepsilon=0.1, lr=0.8, gamma=0.95, episodes=10000)\n",
    "qepsilon = 0.1 # epsilon value for the Q-learning epsilon greedy strategy\n",
    "lr = 0.8 # Q-learning rate\n",
    "qgamma = 0.95 # Q-Learning discount factor\n",
    "episodes = 10000 # number of Q-learning episodes\n",
    "initial = 0 # value to initialize the Q grid\n",
    "\n",
    "# Create Environment\n",
    "env = a4.getEnv(env_id=env_id, rH=rH, rG=rG, rF=rF, \n",
    "                desc=desc, map_name=map_name, \n",
    "                is_slippery=is_slippery,render_initial=True)\n",
    "\n",
    "# Store a representation of the map\n",
    "env_desc = env.desc.astype('<U8')\n",
    "\n",
    "# Store a representation of the state rewards\n",
    "env_rs = a4.getStateReward(env)\n",
    "\n",
    "# Display reward at each state\n",
    "print('\\n--Reward Values at Each State--')\n",
    "a4.matprint(a4.print_value(env_rs,width=size,height=size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951 µs ± 95.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "pi_time = %timeit -o a4.policy_iteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration converged after  7 epochs\n",
      "0.9661  1.4576  2.072  1.4576  \n",
      "1.4576      -5   2.84      -5  \n",
      " 2.072    2.84    3.8      -5  \n",
      "    -5     3.8      5       5  \n",
      "↓  →  ↓  ←  \n",
      "↓  ←  ↓  ←  \n",
      "→  ↓  ↓  ←  \n",
      "←  →  →  ←  \n"
     ]
    }
   ],
   "source": [
    "pi_V, pi_policy, pi_epochs = a4.policy_iteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=True)\n",
    "\n",
    "# Display values\n",
    "a4.matprint(a4.print_value(pi_V))\n",
    "\n",
    "pi_policy_arrows = a4.print_policy(pi_policy, width=size, height=size)\n",
    "\n",
    "# Display policy\n",
    "a4.matprint(pi_policy_arrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.38 ms ± 481 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "vi_time = %timeit -o a4.valueIteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after  91 epochs\n",
      "0.9661  1.4576  2.072  1.4576  \n",
      "1.4576      -5   2.84      -5  \n",
      " 2.072    2.84    3.8      -5  \n",
      "    -5     3.8      5       5  \n",
      "↓  →  ↓  ←  \n",
      "↓  ←  ↓  ←  \n",
      "→  ↓  ↓  ←  \n",
      "←  →  →  ←  \n"
     ]
    }
   ],
   "source": [
    "vi_V, vi_epochs = a4.valueIteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=True)\n",
    "\n",
    "# display value function:\n",
    "a4.matprint(a4.print_value(vi_V))\n",
    "\n",
    "vi_policy = a4.value_to_policy(env, V=vi_V, gamma=gamma)\n",
    "\n",
    "vi_policy_arrows = a4.print_policy(vi_policy, width=size, height=size)\n",
    "# display policy\n",
    "a4.matprint(vi_policy_arrows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43 s ± 98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "Q_time = %timeit -o a4.Qlearning(env, qepsilon, lr, qgamma, episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Q with all options--\n",
      "0.373797  0.603997   0.603997   0.373797  \n",
      "0.364615     -1.95   0.846312   0.601849  \n",
      "0.603982   1.10138   0.603471   0.846216  \n",
      "0.846293    -1.872  -0.738509  -0.738509  \n",
      "0.603997  0.846312      -1.95   0.373797  \n",
      "      -1        -1         -1         -1  \n",
      "   -1.95   1.36987      -1.95   0.845892  \n",
      "      -1        -1         -1         -1  \n",
      "0.846312     -1.95    1.10138   0.603997  \n",
      "0.846312   1.36987    1.36987      -1.95  \n",
      " 1.10138    1.6525      -1.95    1.10138  \n",
      "      -1        -1         -1         -1  \n",
      "      -1        -1         -1         -1  \n",
      "   -1.95   1.36987     1.6525    1.10138  \n",
      " 1.36987    1.6525       1.95    1.36987  \n",
      "       1         1          1          1  \n",
      "\n",
      "--argmax(Q) in grid order--\n",
      " 0.604  0.8463  1.1014  0.8463  \n",
      "0.8463      -1  1.3699      -1  \n",
      "1.1014  1.3699  1.6525      -1  \n",
      "    -1  1.6525    1.95       1  \n",
      "\n",
      "--Policy Matrix--\n",
      "↓  →  ↓  ←  \n",
      "↓  ←  ↓  ←  \n",
      "→  ↓  ↓  ←  \n",
      "←  →  →  ←  \n"
     ]
    }
   ],
   "source": [
    "Q = a4.Qlearning(env, qepsilon, lr, qgamma, episodes)\n",
    "print('--Q with all options--')\n",
    "a4.matprint(Q)\n",
    "\n",
    "maxQ = np.max(Q,axis=1)\n",
    "print('\\n--argmax(Q) in grid order--')\n",
    "a4.matprint(a4.print_value(maxQ))\n",
    "\n",
    "Q_policy = a4.Q_to_policy(Q)\n",
    "\n",
    "Q_policy_arrows = a4.print_policy(Q_policy, width=size, height=size)\n",
    "print('\\n--Policy Matrix--')\n",
    "a4.matprint(Q_policy_arrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--Final position--\n",
      "  (Right)\n",
      "S  F  F  F\n",
      "F  H  F  H\n",
      "F  F  F  H\n",
      "H  F  F  \u001b[41mG\u001b[0m\n",
      "Agent ended up at state 15 after 6 steps\n"
     ]
    }
   ],
   "source": [
    "Q_s, Q_steps = a4.Qlearning_trajectory(env, Q, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default rewards are 1 for the G(oal) and 0 for everything else.\n",
    "\n",
    "Maps are drawn according to the following logic\n",
    "\n",
    "```\n",
    "if desc and map_name are None, \n",
    "   then a default random map is drawn with 8\n",
    "        using frozen_lake.generate_random_map(size=8, p=0.8)\n",
    "elif desc is None and a map_name is given\n",
    "   then a map_name is either '4x4' or '8x8'\n",
    "        and is drawn from the dict MAPS in frozen_lake.py\n",
    "elif desc is given\n",
    "   then it must be in the form of a list with \n",
    "```\n",
    "\n",
    "Default action probabilities are 1/3 chosen action, 1/3 each for right angles to chosen action, and 0 for reverse of chosen action. This is set with `is_slippery=True`. If `is_slippery=False`, then P=1 for chosen action and 0 for all other actions.\n",
    "\n",
    "|ACTION|Value|Symbol|\n",
    "|------|-----|------|\n",
    "|LEFT  | 0   | ←    |\n",
    "|DOWN  | 1   | ↓    |\n",
    "|RIGHT | 2   | →    |\n",
    "|UP    | 3   | ↑    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code: <https://github.com/Twice22/HandsOnRL>\n",
    "- Tutorial: <https://twice22.github.io/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cs7641assn4' has no attribute 'policy_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ee10e9f5c4c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cs7641assn4' has no attribute 'policy_matrix'"
     ]
    }
   ],
   "source": [
    "a4.policy_matrix(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'env_id': [env_id],\n",
    "                        'rH': [rH], \n",
    "                        'rG': [rG], \n",
    "                        'rF': [rF], \n",
    "                        'size': [size], \n",
    "                        'p': [p], \n",
    "                        'desc': [desc], \n",
    "                        'map_name': [map_name],                        \n",
    "                        'is_slippery': [is_slippery],\n",
    "                        'epsilon': [epsilon],\n",
    "                        'gamma': [gamma], \n",
    "                        'max_iter': [max_iter], \n",
    "                        'qepsilon': [qepsilon], \n",
    "                        'lr': [lr], \n",
    "                        'qgamma': [qgamma], \n",
    "                        'episodes': [episodes], \n",
    "                        'env_desc': [env_desc],\n",
    "                        'env_rs': [env_rs],\n",
    "                        'pi_time': [pi_time.average],\n",
    "                        'pi_V': [pi_V],\n",
    "                        'pi_epochs': [pi_epochs],\n",
    "                        'pi_policy': [pi_policy],\n",
    "                        'pi_policy_arrows': [pi_policy_arrows],\n",
    "                        'vi_time': [vi_time.average],\n",
    "                        'vi_V': [vi_V],\n",
    "                        'vi_epochs': [vi_epochs],\n",
    "                        'vi_policy': [vi_policy],\n",
    "                        'vi_policy_arrows': [vi_policy_arrows],\n",
    "                        'Q_time': [Q_time.average],\n",
    "                        'Q': [Q],\n",
    "                        'Q_V': [maxQ],\n",
    "                        'Q_policy': [Q_policy],\n",
    "                        'Q_policy_arrows': [Q_policy_arrows]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>env_id</th>\n",
       "      <th>rH</th>\n",
       "      <th>rG</th>\n",
       "      <th>rF</th>\n",
       "      <th>size</th>\n",
       "      <th>p</th>\n",
       "      <th>desc</th>\n",
       "      <th>map_name</th>\n",
       "      <th>is_slippery</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>...</th>\n",
       "      <th>vi_time</th>\n",
       "      <th>vi_V</th>\n",
       "      <th>vi_epochs</th>\n",
       "      <th>vi_policy</th>\n",
       "      <th>vi_policy_arrows</th>\n",
       "      <th>Q_time</th>\n",
       "      <th>Q</th>\n",
       "      <th>Q_V</th>\n",
       "      <th>Q_policy</th>\n",
       "      <th>Q_policy_arrows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Deterministic-4x4-FrozenLake-v0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>None</td>\n",
       "      <td>4x4</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006382</td>\n",
       "      <td>[0.9660799905143127, 1.4575999905143127, 2.071...</td>\n",
       "      <td>91</td>\n",
       "      <td>[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]</td>\n",
       "      <td>[[↓, →, ↓, ←], [↓, ←, ↓, ←], [→, ↓, ↓, ←], [←,...</td>\n",
       "      <td>1.428201</td>\n",
       "      <td>[[0.37379674921874956, 0.6039965781249996, 0.6...</td>\n",
       "      <td>[0.6039965781249996, 0.8463121874999997, 1.101...</td>\n",
       "      <td>[1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]</td>\n",
       "      <td>[[↓, →, ↓, ←], [↓, ←, ↓, ←], [→, ↓, ↓, ←], [←,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            env_id  rH  rG   rF  size    p  desc map_name  \\\n",
       "0  Deterministic-4x4-FrozenLake-v0  -1   1 -0.2     4  0.8  None      4x4   \n",
       "\n",
       "   is_slippery       epsilon  \\\n",
       "0        False  1.000000e-08   \n",
       "\n",
       "                         ...                           vi_time  \\\n",
       "0                        ...                          0.006382   \n",
       "\n",
       "                                                vi_V  vi_epochs  \\\n",
       "0  [0.9660799905143127, 1.4575999905143127, 2.071...         91   \n",
       "\n",
       "                                          vi_policy  \\\n",
       "0  [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]   \n",
       "\n",
       "                                    vi_policy_arrows    Q_time  \\\n",
       "0  [[↓, →, ↓, ←], [↓, ←, ↓, ←], [→, ↓, ↓, ←], [←,...  1.428201   \n",
       "\n",
       "                                                   Q  \\\n",
       "0  [[0.37379674921874956, 0.6039965781249996, 0.6...   \n",
       "\n",
       "                                                 Q_V  \\\n",
       "0  [0.6039965781249996, 0.8463121874999997, 1.101...   \n",
       "\n",
       "                                           Q_policy  \\\n",
       "0  [1, 2, 1, 0, 1, 0, 1, 0, 2, 1, 1, 0, 0, 2, 2, 0]   \n",
       "\n",
       "                                     Q_policy_arrows  \n",
       "0  [[↓, →, ↓, ←], [↓, ←, ↓, ←], [→, ↓, ↓, ←], [←,...  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(results)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
