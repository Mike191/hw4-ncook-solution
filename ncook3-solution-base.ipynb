{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cs7641assn4 as a4\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Board--\n",
      "\n",
      "\u001b[41mS\u001b[0m  F  F  F\n",
      "F  H  F  H\n",
      "F  F  F  H\n",
      "H  F  F  G\n",
      "\n",
      "--Actions for Position to the Left of the Goal--\n",
      "{0: [(1.0, 13, -0.2, False)],\n",
      " 1: [(1.0, 14, -0.2, False)],\n",
      " 2: [(1.0, 15, 1, True)],\n",
      " 3: [(1.0, 10, -0.2, False)]}\n",
      "\n",
      "--Reward Values at Each State--\n",
      "-0.2  -0.2  -0.2  -0.2  \n",
      "-0.2    -1  -0.2    -1  \n",
      "-0.2  -0.2  -0.2    -1  \n",
      "  -1  -0.2  -0.2     1  \n"
     ]
    }
   ],
   "source": [
    "id = 'Deterministic-4x4-FrozenLake-v0' # string identifier for environment, arbitrary label\n",
    "rH = -1 #-5 # reward for H(ole)\n",
    "rG = 1 # 10 # reward for G(oal)\n",
    "rF = -0.2# reward includes S(tart) and F(rozen)\n",
    "size = 4 # height and width of square gridworld\n",
    "p = 0.8 # if generating a random map probability that a grid will be F(rozen)\n",
    "desc = None # frozen_lake.generate_random_map(size=size, p=p)\n",
    "map_name = 'x'.join([str(size)]*2) # None\n",
    "is_slippery = False\n",
    "\n",
    "\n",
    "epsilon = 1e-8 # convergence threshold for policy/value iteration\n",
    "gamma = 0.8 # discount parameter for past policy/value iterations\n",
    "max_iter = 10000 # maximum iterations for slowly converging policy/value iteration \n",
    "\n",
    "# Qlearning(env, rH=0, rG=1, rF=0, qepsilon=0.1, lr=0.8, gamma=0.95, episodes=10000)\n",
    "qepsilon = 0.1 # epsilon value for the Q-learning epsilon greedy strategy\n",
    "lr = 0.8 # Q-learning rate\n",
    "qgamma = 0.95 # Q-Learning discount factor\n",
    "episodes = 10000 # number of Q-learning episodes\n",
    "\n",
    "# Create Environment\n",
    "env = a4.getEnv(id=id, rH=rH, rG=rG, rF=rF, desc=desc, map_name=map_name, is_slippery=is_slippery,render_initial=True)\n",
    "\n",
    "# Display reward at each state\n",
    "print('\\n--Reward Values at Each State--')\n",
    "a4.matprint(a4.print_value(a4.getStateReward(env),width=size,height=size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%%timeit\n",
    "V, pi, epochs = a4.policy_iteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=False)\n",
    "```\n",
    "\n",
    "670 µs ± 55.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration converged after  7 epochs\n",
      "0.9661  1.4576  2.072  1.4576  \n",
      "1.4576      -5   2.84      -5  \n",
      " 2.072    2.84    3.8      -5  \n",
      "    -5     3.8      5       5  \n",
      "↓  →  ↓  ←  \n",
      "↓  ←  ↓  ←  \n",
      "→  ↓  ↓  ←  \n",
      "←  →  →  ←  \n"
     ]
    }
   ],
   "source": [
    "V, pi, epochs = a4.policy_iteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=True)\n",
    "\n",
    "# Display values\n",
    "a4.matprint(a4.print_value(V))\n",
    "\n",
    "# Display policy\n",
    "a4.matprint(a4.print_policy(pi, width=size, height=size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "%%timeit\n",
    "V, epochs = a4.valueIteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=False)\n",
    "```\n",
    "\n",
    "6.38 ms ± 658 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value iteration converged after  91 epochs\n",
      "0.9661  1.4576  2.072  1.4576  \n",
      "1.4576      -5   2.84      -5  \n",
      " 2.072    2.84    3.8      -5  \n",
      "    -5     3.8      5       5  \n",
      "↓  →  ↓  ←  \n",
      "↓  ←  ↓  ←  \n",
      "→  ↓  ↓  ←  \n",
      "←  →  →  ←  \n"
     ]
    }
   ],
   "source": [
    "V, epochs = a4.valueIteration(env, epsilon=epsilon, gamma=gamma, max_iter=max_iter, report=True)\n",
    "\n",
    "# display value function:\n",
    "a4.matprint(a4.print_value(V))\n",
    "\n",
    "pol = a4.value_to_policy(env, V=V, gamma=gamma)\n",
    "\n",
    "# display policy\n",
    "a4.matprint(a4.print_policy(pol, width=size, height=size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Q with all options--\n",
      "0.373797  0.603997   0.603997   0.373797  \n",
      "0.373797     -1.95   0.846312   0.603589  \n",
      "0.594355   1.10138   0.603991   0.839005  \n",
      "0.846312     -1.56  -0.602614  -0.602614  \n",
      "0.603997  0.846312      -1.95   0.373797  \n",
      "      -1        -1         -1         -1  \n",
      "-1.94998   1.36987      -1.95   0.846309  \n",
      "      -1        -1         -1         -1  \n",
      "0.846312     -1.95    1.10138   0.603997  \n",
      "0.846312   1.36987    1.36987      -1.95  \n",
      " 1.10138    1.6525      -1.95    1.10138  \n",
      "      -1        -1         -1         -1  \n",
      "      -1        -1         -1         -1  \n",
      "   -1.95   1.36987     1.6525    1.10138  \n",
      " 1.36987    1.6525       1.95    1.36987  \n",
      "       1         1          1          1  \n",
      "\n",
      "--argmax(Q) in grid order--\n",
      " 0.604  0.8463  1.1014  0.8463  \n",
      "0.8463      -1  1.3699      -1  \n",
      "1.1014  1.3699  1.6525      -1  \n",
      "    -1  1.6525    1.95       1  \n",
      "\n",
      "--Final position--\n",
      "  (Right)\n",
      "S  F  F  F\n",
      "F  H  F  H\n",
      "F  F  F  H\n",
      "H  F  F  \u001b[41mG\u001b[0m\n",
      "Agent ended up at state 15 after 6 steps\n",
      "\n",
      "--Policy Matrix--\n",
      "↓  →  ↓  ←  \n",
      "↓  ←  ↓  ←  \n",
      "→  ↓  ↓  ←  \n",
      "←  →  →  ←  \n"
     ]
    }
   ],
   "source": [
    "Q = a4.Qlearning(env, qepsilon, lr, qgamma, episodes)\n",
    "print('--Q with all options--')\n",
    "a4.matprint(Q)\n",
    "\n",
    "maxQ = Q[range(len(Q)),argmax(Q,axis=1)]\n",
    "print('\\n--argmax(Q) in grid order--')\n",
    "a4.matprint(a4.print_value(maxQ))\n",
    "\n",
    "s, steps = a4.Qlearning_trajectory(env, Q, render=False)\n",
    "print('\\n--Policy Matrix--')\n",
    "a4.matprint(a4.policy_matrix(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default rewards are 1 for the G(oal) and 0 for everything else.\n",
    "\n",
    "Maps are drawn according to the following logic\n",
    "\n",
    "```\n",
    "if desc and map_name are None, \n",
    "   then a default random map is drawn with 8\n",
    "        using frozen_lake.generate_random_map(size=8, p=0.8)\n",
    "elif desc is None and a map_name is given\n",
    "   then a map_name is either '4x4' or '8x8'\n",
    "        and is drawn from the dict MAPS in frozen_lake.py\n",
    "elif desc is given\n",
    "   then it must be in the form of a list with \n",
    "```\n",
    "\n",
    "Default action probabilities are 1/3 chosen action, 1/3 each for right angles to chosen action, and 0 for reverse of chosen action. This is set with `is_slippery=True`. If `is_slippery=False`, then P=1 for chosen action and 0 for all other actions.\n",
    "\n",
    "|ACTION|Value|Symbol|\n",
    "|------|-----|------|\n",
    "|LEFT  | 0   | ←    |\n",
    "|DOWN  | 1   | ↓    |\n",
    "|RIGHT | 2   | →    |\n",
    "|UP    | 3   | ↑    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code: <https://github.com/Twice22/HandsOnRL>\n",
    "- Tutorial: <https://twice22.github.io/>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
